---
sidebar_position: 8
---
import useBaseUrl from '@docusaurus/useBaseUrl';
import Admonition from '@theme/Admonition';

# 8. Data Engineering

In this section two Databricks Products are positioned:
* Spark Declarative Pipelines (previously known as Lakeflow Pipelines or DLT).
* Lakeflow Jobs.

<iframe
  width="560"
  height="315"
  src="https://www.youtube.com/embed/n8XWOr6zIPo"
></iframe>

## How to process and transform data?

### Spark Declarative Pipelines (SDP)

* Supports SQL or Python.
* **Process multiple sources simultaneously** whether streaming from Kafka, batch loading from cloud storage, or querying external databases.
* **Built-in incremental processing** intelligently tracks changes and processes only new or modified data, dramatically reducing compute costs and pipeline runtimes.
* **Data quality is enforced through declarative expectations** that you define inline with your transformations.
* Integrated with **Unity Catalog** (everything on Databricks is Unity Catalog).

<iframe
  width="560"
  height="315"
  src="https://www.youtube.com/embed/bsPBdKnZkr0"
></iframe>

### SDP in action!

Install and run the following demo:

* [CDC Pipeline With Delta
](https://www.databricks.com/resources/demos/tutorials/data-engineering/cdc-pipeline-with-delta?itm_data=demo_center).
* By default it targets the **main** catalog, you can specify the catalog name as a parameter:

```python
import dbdemos

catalog_name = "dev"
dbdemos.install('cdc-pipeline', catalog = catalog_name)
```

### References for SDP development

#### SDP Features

* [Load data in pipelines](https://docs.databricks.com/aws/en/ldp/load).
    * Data in cloud object storage is processed using [AutoLoader](https://docs.databricks.com/aws/en/ingestion/cloud-object-storage/auto-loader/).
* [Load and process data incrementally with Lakeflow Spark Declarative Pipelines flows
](https://docs.databricks.com/aws/en/ldp/flows).
* [Transform data with pipelines
](https://docs.databricks.com/aws/en/ldp/transform).
* [Manage data quality with pipeline expectations
](https://docs.databricks.com/aws/en/ldp/expectations).

#### Python 
* [Develop SDP with Python
](https://docs.databricks.com/aws/en/ldp/developer/python-dev).
* [SDP Python language reference
](https://docs.databricks.com/aws/en/ldp/developer/python-ref).
    
#### SQL
    * [Develop SDP with SQL](https://docs.databricks.com/aws/en/ldp/developer/sql-dev).
    * [SDP SQL language reference](https://docs.databricks.com/aws/en/ldp/developer/sql-ref).

#### Tutorials and code examples

* [Tutorial: Build an ETL pipeline with Lakeflow Spark Declarative Pipelines
](https://docs.databricks.com/aws/en/getting-started/data-pipeline-get-started#step-2-develop-your-pipeline-logic).
* [Sample pipeline notebook](https://github.com/ivancalvo-dbxs/medallion-pipeline-dabs/blob/main/src/pipeline_notebook.ipynb).
    * [Youtube walkthrough of the repo and project](https://www.youtube.com/watch?v=W8ucHwzwdhc).
    * Focus and use the notebook as a development reference, ignore the repo and other files.
    * **This is a good refence on how to build a Medallion Architecture on SDP**.

### My SDP is running smoothly now, what's next?

* Schedule the pipeline execution (and other tasks if required) with Lakeflow Jobs.

## How to setup orchestration?

### Lakeflow Jobs

On Databricks, everything related to:
* Orchestration.
* Tasks, DAGs and dependencies. 
* Schedule of execution and triggers.
* Retries and notifications (success, failures).

Should be configured as a [Lakeflow Job (AKA Databricks Jobs)](https://docs.databricks.com/aws/en/jobs). 

### Jobs in action!

<iframe
  width="560"
  height="315"
  src="https://www.youtube.com/embed/BUFDNFA_AgA"
></iframe>

:::info Databricks Workflows 
* Databricks Jobs (previously known as Databricks Workflows).
* If you read **Workflow** just remember that it is refering to a **Job**.
:::

<iframe
  width="560"
  height="315"
  src="https://www.youtube.com/embed/0TOrtVEYeV8"
></iframe>

### Create the first Job

* [Create your first Lakeflow Job
](https://docs.databricks.com/aws/en/jobs/jobs-quickstart)
    * The pipeline created on the previous step should be orchestrated from a Job.